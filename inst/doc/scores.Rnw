% -*- mode: noweb; ess-noweb-default-code-mode: R-mode; -*-
\documentclass[nojss]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}
\definecolor{updatecolor}{rgb}{0,0.4,0}
\newcommand{\update}[1]{\marginpar{UPDATE}{\color{updatecolor}#1}}

\author{Hannah Frick\\Universit\"at Innsbruck \And
	Carolin Strobl\\Universit\"at Z\"urich \And
	Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Hannah Frick, Carolin Strobl, Achim Zeileis}


\title{Rasch Mixture Models for DIF Detection:\\A Comparison of Old and New Score Specifications}
\Plaintitle{Rasch Mixture Models for DIF Detection: A Comparison of Old and New Score Specifications}
\Shorttitle{Rasch Mixture Models for DIF Detection}



\Keywords{mixed Rasch model, Rasch mixture model, DIF detection, score distribution}
\Plainkeywords{mixed Rasch model, Rasch mixture model, DIF detection, score distribution}

\Abstract{
  Rasch mixture models can be a useful tool when checking the assumption of 
  measurement invariance for a single Rasch model. They provide advantages 
  compared to manifest DIF tests when the DIF groups are only weakly correlated 
  with the manifest covariates available. Unlike in single Rasch models, 
  estimation of Rasch mixture models is sensitive to the specification of the 
  ability distribution even when the conditional maximum likelihood approach is 
  used. It is demonstrated in a simulation study how differences in ability can 
  influence the latent classes of a Rasch mixture model. If the aim is only DIF
  detection, it is not of interest to uncover such ability differences as one is
  only interested in a latent group structure regarding the item difficulties. 
  To avoid any confounding effect of ability differences (or impact), a score 
  distribution for the Rasch mixture model is introduced here which is 
  restricted to be equal across latent classes. This causes the estimation of 
  the Rasch mixture model to be independent of the ability distribution and thus
  restricts the mixture to be sensitive to latent structure in the item 
  difficulties only. Its usefulness is demonstrated in a simulation study and 
  its application is illustrated in a study of verbal aggression.
}

\Address{
  Hannah Frick, Achim Zeileis\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Hannah.Frick@uibk.ac.at}, \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://eeecon.uibk.ac.at/~frick/}, \url{http://eeecon.uibk.ac.at/~zeileis/}\\

  Carolin Strobl\\
  Department of Psychology\\
  Universit\"at Z\"urich\\
  Binzm\"uhlestr.~14\\
  8050 Z\"urich, Switzerland\\
  E-mail: \email{Carolin.Strobl@psychologie.uzh.ch}\\
  URL: \url{http://www.psychologie.uzh.ch/methoden.html}
}

%% Sweave/vignette information and metadata
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}
%\VignetteIndexEntry{On the Specification of the Score Distribution in Rasch Mixture Models}
%\VignetteDepends{graphics, stats, methods, Formula, flexmix, psychotools, psychomix, lmtest}
%\VignetteKeywords{mixed Rasch model, Rasch mixture model, DIF detection, score distribution}
%\VignettePackage{psychomix}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ")
library("psychomix")
set.seed(1090)
cache <- FALSE
@


\begin{document}

%\vspace*{-0.35cm}

\section{Introduction} \label{sec:intro}

Based on the Rasch model \citep{psychomix:Rasch:1960}, 
\cite{psychomix:Rost:1990} introduced what he called the \dquote{mixed Rasch 
  model}, a combination of a latent class approach and a latent trait approach 
to model qualitative and quantitative ability differences. As suggested by 
\cite{psychomix:Rost:1990}, it can also be used to examine the fit of Rasch 
model and check for violations of measurement invariance such as differential 
item functioning (DIF). It has since been extended by 
\cite{psychomix:Rost+VonDavier:1995} to different score distributions and by 
\cite{psychomix:Rost:1991} and \cite{psychomix:vonDavier+Rost:1995} to 
polytomous responses. The so-called \dquote{mixed ordinal Rasch model} is a 
mixture of partial credit models \citep[PCM,][]{psychomix:Masters:1982} and 
includes a mixture of rating scale models \citep[RSM,][]{psychomix:Andrich:1978}
as a special case.

The original dichotomous model -- here called Rasch mixture model to avoid 
confusion with mixed (effects) models and instead highlight its relation to 
mixture models -- as well as its polytomous version have been applied in a 
variety of fields. \cite{psychomix:Zickar+Gibby+Robie:2004} use a mixture PCM to
detect faking in personality questionnaires, while 
\cite{psychomix:Hong+Min:2007} identify three types/classes of depressed 
behavior by applying a mixture RSM to a self-rating depression scale. Another 
vast field of application are tests in educational measurement. 
\cite{psychomix:Baghaei+Carstensen:2013} identify different reader types from a 
reading comprehension test using a Rasch mixture model. 
\cite{psychomix:Maij-deMeij+Kelderman+vanderFlier:2010} also apply a Rasch 
mixture model to identify latent groups in a vocabulary test. 
\cite{psychomix:Cohen+Bolt:2005} use a Rasch mixture model to detect DIF in a 
mathematics placement test.

Rasch mixture models constitute a legitimate alternative to manifest DIF tests,
as \cite{psychomix:Maij-deMeij+Kelderman+vanderFlier:2010} show that mixture
models are more suitable to detect DIF if the \dquote{true source of bias} is 
a latent grouping variable. The simulation study by 
\cite{psychomix:Preinerstorfer+Formann:2011} suggests that parameter recovery
works reasonably well for Rasch mixture models. While they did not study in 
detail the influence of DIF effect size or the effect of different ability 
distributions, they deem such differences relevant for practical concern but 
leave it to further research to establish just how strongly they influence 
estimation accuracy.

As the Rasch model is based on two aspects, subject ability and item difficulty,
Rasch mixture models are sensitive not only to differences in the item 
difficulties -- as in DIF -- but also to differences in abilities. Such 
differences in abilities are usually called impact and do not infringe on 
measurement invariance \citep{psychomix:Ackerman:1992}. In practice, when 
developing a psychological test, one often follows two main steps. First, the 
item parameters are estimated, e.g., by means of the conditional maximum 
likelihood (CML) approach, checked for model violations and problematic items 
are possibly excluded or modified. Second, the final set of items is used to 
estimate person abilities. The main advantage of the CML approach is that, for a
single Rasch model, the estimation and check of item difficulties are 
(conditionally) independent of the abilities and their distribution. However, 
in a Rasch mixture model, the estimation of the item difficulties is not 
independent of this second aspect, even when employing the CML approach. 
\cite{psychomix:DeMars+Lau:2011} find that a difference in mean ability between 
DIF groups affects the estimation of the DIF effect sizes. Similarly, inflated 
type I error rates also occur in other DIF detection methods if impact is 
present, e.g., the Mantel-Haenszel and logistic regression precedures 
\citep{psychomix:Li+Brooks+Johanson:2012, psychomix:DeMars:2010}.

Here, a simulations study is conducted to illustrate how Rasch mixture models 
react to impact, either alone or in combination with DIF. When using a Rasch 
mixture model for DIF detection, an influence of sole impact on the mixture is
undesirable as the goal is to uncover DIF groups based on item difficulties, not
impact groups based on abilities.

To avoid such confounding effects of impact, we propose a Rasch mixture model
specifically designed to detect DIF -- regardless of whether or not impact is 
present and in which form.

In the following, we briefly discuss the Rasch model and Rasch mixture models to
explain why the latter are sensitive to the specification of the score 
distribution despite employing a conditional maximum likelihood approach for 
estimation. This Section~\ref{sec:theory} is concluded with our suggested score 
distribution. We illustrate and discuss the behavior of Rasch mixture models 
with different options for the score distribution in a Monte Carlo study in 
Section~\ref{sec:simulation}. The suggested approach for DIF detection via Rasch
mixture models is illustrated through an empirical application to a study on 
verbally aggressive behavior in Section~\ref{sec:application}. Concluding 
remarks are provided in Section~\ref{sec:conclusion}.

\pagebreak

\section{Theory} \label{sec:theory}


\subsection{The Rasch model} \label{sec:Rasch}

The Rasch model, introduced by Georg \cite{psychomix:Rasch:1960}, models the
probability for a binary response~$y_{ij} \in \{0, 1\}$ by subject~$i$ to
item~$j$ as dependent on the subject's ability~$\theta_i$ and the item's
difficulty~$\beta_j$. Assuming independence between items given the subject, the
probability for observing a vector $y_i = (y_{i1}, \dots, y_{im})^\top$ with
responses to all $m$~items by subject~$i$ can be written as
%
\begin{equation}
  P(Y_{i} = y_{i} | \theta_i, \beta) ~=~ \prod_{j=1}^{m}
  \frac{\exp \{y_{ij}(\theta_i - \beta_j)\}}{1 + \exp \{\theta_i - \beta_j\}},
  \label{eq:RM:base}
\end{equation}
%
depending on the subject's ability $\theta_i$ and the vector of all item 
difficulties $\beta = (\beta_1, \dots, \beta_m)^\top$.
Capital letters denote random variables and lower case letters denote their
realizations.

Since joint maximum likelihood (JML) estimation of all abilities and 
difficulties is not consistent for a fixed number of items $m$ 
\citep{psychomix:Molenaar:1995a}, conditional maximum likelihood (CML) 
estimation is employed here. This exploits that the number of correctly scored 
items, the so-called raw score $R_i = \sum_{j = 1}^{m}{Y_{ij}}$, is a sufficient 
statistic for the ability $\theta_i$ \citep{psychomix:Molenaar:1995a}.
Therefore, the answer probability from Equation~\ref{eq:RM:base} can be split
into two factors where the first factor, is conditionally independent of 
$\theta_i$:
%
\begin{eqnarray*}
  \label{eq:RM:fac}
  P(Y_{i} = y_{i} | \theta_i, \beta)
    & = & P(Y_i = y_i | r_i, \theta_i, \beta) ~ P(R_i = r_i | \theta_i, \beta) \\
  \label{eq:RM:hg}
    & = & \underbrace{P(Y_i = y_i | r_i, \beta)}_{h(y_i | r_i, \beta)} ~
          \underbrace{P(R_i = r_i | \theta_i, \beta)}_{g(r_i | \theta_i, \beta)}
\end{eqnarray*} 
%
Due to this separation, consistent estimates of the item parameters $\beta$ can
be obtained by maximizing only the conditional part of the likelihood 
$h(\cdot)$:
% 
\begin{equation}
  h(y_i| r, \beta) ~=~ \frac
    {\exp\{- \sum_{j = 1}^{m}{y_{ij} \beta_j}\}}
    {\gamma_{r_i}(\beta)}, \label{eq:RM:h}
\end{equation}
%
with $\gamma_{j}(\cdot)$ denoting the elementary symmetric function of 
order~$j$. The resulting CML estimates $\hat \beta$ are consistent, 
asymptotically normal, and asymptotically efficient 
\citep{psychomix:Molenaar:1995a}.

If not only the conditional likelihood but the full likelihood is of interest
-- as in Rasch mixture models -- then the score distribution $g(\cdot)$ needs
to be specified as well. The approach used by \cite{psychomix:Rost:1990} and
\cite{psychomix:Rost+VonDavier:1995} is to employ some distribution for the
raw scores $r_i$ based on a set of auxiliary parameters $\delta$. Then the
probability density function for $y_i$ can be written as:
%
\begin{equation} \label{eq:RM:f}
f(y_i | \beta, \delta) ~=~ h(y_i | r_i, \beta) ~ g(r_i | \delta).
\end{equation}
%
Based on this density, the following subsections first introduce mixture Rasch 
models in general and then discuss several choices for $g(\cdot)$. CML 
estimation is used throughout for estimating the Rasch model, i.e., the 
conditional likelihood $h(\cdot)$ is always specified by Equation~\ref{eq:RM:h}.


\pagebreak

\subsection{Rasch mixture models}\label{sec:RMM}

Mixture models are essentially a weighted sum over several components,
i.e., here over several Rasch models.Using the Rasch model density function from
Equation~\ref{eq:RM:f} the likelihood $L(\cdot)$ of a Rasch mixture model 
with $K$~components for data from $n$~respondents is given by
%
\begin{eqnarray}
  L(\pi^{(1)}, \dots, \pi^{(K)}, \beta^{(1)}, \dots, \beta^{(K)}, \delta^{(1)}, \dots, \delta^{(K)}) 
  &=& \prod_{i=1}^{n}{\sum_{k=1}^{K}{\pi^{(k)} f(y_i | \beta^{(k)}, \delta^{(k)})}}  \label{eq:RMM:f} \nonumber\\
  &=& \prod_{i=1}^{n}{\sum_{k=1}^{K}{\pi^{(k)} h(y_i | r_i, \beta^{(k)}) ~ g(r_i | \delta^{(k)})}}. \label{eq:RMM:hg} 
\end{eqnarray}
%
where the $(k)$-superscript denotes the component-specific parameters: the 
component weight $\pi^{(k)}$, the component-specific item parameters 
$\beta^{(k)}$, and the component-specific score parameters $\delta^{(k)}$ for 
$k = 1, \dots, K$.

This kind of likelihood can be maximized via the expectation-maximization (EM)
algorithm \citep{psychomix:Dempster+Laird+Rubin:1977} which alternates between
maximizing the component-specific likelihoods for obtaining parameter estimates
and computing expectations for each observations belonging to each cluster.

More formally, given (initial) estimates for the model parameters
$\hat{\pi}^{(k)}$, $\hat{\beta}^{(k)}$, $\hat{\delta}^{(k)}$ for all components
$k = 1, \dots, K$, posterior probabilities of each observation~$i$ belonging to
a component, or latent class,~$k$ are calculated in the E-step. This is simply 
$i$'s relative contribution to component~$k$ compared to the sum of all its 
contributions:
% 
\begin{equation}
  \hat{p}_{ik}
    ~=~ \frac{\hat{\pi}^{(k)} ~ f(y_i | \hat{\beta}^{(k)}, \hat{\delta}^{(k)})}%
       {\sum_{\ell=1}^{K} \hat{\pi}^{(\ell)} ~ f(y_i | \hat{\beta}^{(\ell)}, \hat{\delta}^{(\ell)})}
    ~=~ \frac{\hat{\pi}^{(k)} ~ h(y_i | r_i, \hat{\beta}^{(k)}) ~ g(r_i | \hat{\delta}^{(k)})}%
       {\sum_{\ell=1}^{K} \hat{\pi}^{(\ell)} ~ h(y_i | r_i, \hat{\beta}^{(\ell)}) ~ g(r_i | \hat{\delta}^{(\ell)})} \; . \label{eq:RMM:Estep:hg}
\end{equation}
% 
In the M-step of the algorithm, these posterior probabilities are used as the
weights in a weighted ML estimation of the model parameters. This way, an
observation deemed unlikely to belong to a certain latent class does not 
contribute strongly to its estimation. Estimation can be done separately for 
each latent class. Using CML estimation for the Rasch Model, the estimation of
item and score parameters can again be done separately. For all components 
$k = 1, \dots, K$:
% 
\begin{eqnarray}
  (\hat{\beta}^{(k)}, \hat{\delta}^{(k)}) 
  & = & \argmax_{\beta^{(k)}, \delta^{(k)}} \sum_{i=1}^{n}{\hat{p}_{ik}
    \log f(y_i | \beta^{(k)}, \delta^{(k)})}
  \label{eq:RMM:Mstep} \nonumber\\
  & = & \left\{
    \argmax_{\beta^{(k)}} \sum_{i=1}^{n}{\hat{p}_{ik} \log h(y_i | r_i, \beta^{(k)})};~
    \argmax_{\delta^{(k)}} \sum_{i=1}^{n}{\hat{p}_{ik} \log g(r_i | \delta^{(k)})}
  \right\}. \label{eq:RMM:Mstep:sep}
\end{eqnarray}
%
Estimates of the class probabilities can be obtained from the posterior 
probabilities by averaging:
%
\begin{equation}
  \hat{\pi}^{(k)} = \frac{1}{n} \sum_{i=1}^{n}{\hat{p}_{ik}}. \label{eq:RMM:Mstep:pi}  
\end{equation}
%
The E-step (Equation~\ref{eq:RMM:Estep:hg}) and M-step 
(Equations~\ref{eq:RMM:Mstep:sep} and~\ref{eq:RMM:Mstep:pi}) are iterated until 
convergence, always updating either the weights based on current estimates for 
the model parameters or vice versa.

Note that the above implicitly assumes that the number of latent classes~$K$ is
given or known. However, this is typically not the case in practice and $K$ 
needs to be chosen based on the data. As $K$ is not a model parameter -- 
regularity conditions for the likelihood ratio test are not fulfilled
\citep[][Chapter~6.4]{psychomix:McLachlan+Peel:2000} -- it is often chosen via 
some information criterion that balances goodness of fit
(via the likelihood) with a penalty for the number of model parameters. In the 
following, the BIC 
\citep[Bayesian information criterion,][]{psychomix:Schwarz:1978} is used, which
\cite{psychomix:Li+Cohen+Kim:2009} found to be a suitable model selection method
for dichotomous mixture item response theory models.


\subsection{Score distribution} \label{sec:scores}

In a single Rasch model, the estimation of the item parameters is invariant to
the score distribution because of the separation in Equation~\ref{eq:RM:f}.
In the mixture context, this invariance property holds only \emph{given the 
  weights} in Equation~\ref{eq:RMM:Mstep:sep}. However, these posterior weights
depend on the full Rasch likelihood, including the score distribution 
(Equation~\ref{eq:RMM:Estep:hg}). Therefore, the estimation of the item 
parameters in a Rasch mixture model is \emph{not} independent of the score 
distribution for $K > 1$, even if the CML approach is employed. Hence, it is 
important to consider the specification of the score distribution when 
estimating Rasch mixture models and to assess the consequences of potential 
misspecifications.

\subsubsection{Saturated and mean-variance specification}

In his introduction of the Rasch mixture model, \cite{psychomix:Rost:1990}
suggests a discrete probability distribution on the scores with a separate
parameter for each possible score. This requires $m - 2$ parameters per latent 
class as the probabilities need to sum to $1$ (and the extreme scores, $r = 0$ 
and $r = m$, do not contribute to the likelihood).

Realizing that this saturated specification requires a potentially rather large
number of parameters, \cite{psychomix:Rost+VonDavier:1995} suggest a parametric
distribution with one parameter each for mean and variance.

Details on both specifications can be found in \cite{psychomix:Rost:1990} and 
\cite{psychomix:Rost+VonDavier:1995}, respectively. Here, the notation of 
\cite{psychomix:Frick+Strobl+Leisch:2012} is adopted, which expresses both 
specifications in a unified way through a conditional logit model for the 
score $r = 1, \dots, m-1$:
% 
\begin{equation*}
  \label{eq:scores:condLogit}
  g(r | \delta^{(k)}) ~=~
      \frac{\exp\{ z_{r}^\top \delta^{(k)} \}}
           {\sum_{j=1}^{m-1}{\exp\{z_{j}^\top \delta^{(k)} \}}},
\end{equation*}
%
with different choices for $z_r$ leading to the saturated and mean-variance
specification, respectively. For the former, the regressor vector is 
$(m-2)$-dimensional with
%
\begin{equation*}
  \label{eq:scores:saturated}
  z_{r} = (0, \ldots, 0, 1, 0, \ldots, 0)^\top
\end{equation*}
% 
and the 1 at position $r - 1$. Consequently, if $r = 1$, $z_{r}$ is a vector
of zeros. For the mean-variance specification, the regressor vector is
$2$-dimensional and given by
%
\begin{equation*}
  \label{eq:scores:meanvar}
  z_{r} = \left( \frac{r}{m}, \frac{4 r (m - r)}{m^2} \right)^\top.
\end{equation*}

\subsubsection{Restricted specification}

To obtain independence of the item parameter estimates from the specification
of the score distribution in the Rasch mixture model from 
Equation~\ref{eq:RMM:hg}, we propose a novel specification of the score 
distribution in the Rasch mixture model. We suggest restricting the score 
parameters $\delta^{(k)}$ to be equal across the latent classes: 
%
\begin{equation*}
  \label{eq:scores:restricted}
  g(r | \delta^{(k)}) = g(r | \delta) \qquad (k = 1, \dots, K).
\end{equation*}
%
The independence of the item parameter estimates can then be seen easily from
the definition of the posterior weights (Equation~\ref{eq:RMM:Estep:hg}): 
$g(\cdot)$ can be moved out of the sum and then cancels out. Thus, the 
$\hat p_{ik}$ depend only on $\hat \pi^{(k)}$ and $\hat \beta^{(k)}$ but not 
$\hat \delta^{(k)}$. Therefore, the component weights and component-specific item
parameters can be estimated without any specification of the score distribution.
To complete the likelihood, a score distribution is simply fitted to the 
full-sample scores. 

Consequently, using a restricted score specification, the likelihood of the 
Rasch mixture model (Equation~\ref{eq:RMM:hg}) can be split into two factors: 
one depending on the general score parameters $\delta$ and the other one 
depending on latent class-specific prior probabilities 
$\pi^{(1)}, \ldots, \pi^{(K)}$ and item difficulties 
$\beta^{(1)}, \ldots, \beta^{(K)}$. The mixture is then only based on latent
structure in the item difficulties, not on latent structure in both difficulties
and scores. Also, the selection of the number of classes~$K$ is \emph{not} 
affected by the specification of the score distribution $g(\cdot)$.

\subsubsection{Overview}

The different specifications of the score distribution vary in their properties
and implications for the whole Rasch mixture model.
\begin{itemize}
  \item The saturated model is very flexible. It can model any shape and is thus
    never misspecified. However, it needs a potentially large number of
    parameters which can be challenging in model estimation and selection.

  \item The mean-variance specification of the score model is more parsimonious
    as it only requires two parameters per latent class. While this is
    convenient for model fit and selection, it also comes at a cost: since it
    can only model unimodal or U-shaped distributions 
    \citep[see][]{psychomix:Rost+VonDavier:1995}, it is partially misspecified 
    if the score distribution is actually multimodal.

  \item A restricted score model is even more parsimonious. Therefore, the same
    advantages in model fit and selection apply. Furthermore, it is invariant to
    the latent structure in the score distribution. If a Rasch mixture model is 
    used for DIF detection, this is favorable as only differences in the item
    difficulties influence the mixture. However, it is partially misspecified if
    the latent structure in the scores and item difficulties coincides.
\end{itemize}


\section{Monte Carlo study} \label{sec:simulation}

The simple question \emph{\dquote{DIF or no DIF?}} leads to the question 
whether the Rasch mixture model is suitable as a tool to detect such violations 
of measurement invariance. As the score distribution influences the estimation 
of the Rasch mixture model in general, it is of particular interest how it 
influences the estimation of the number of latent classes, the measure used to 
determine Rasch scalability.
        
To illustrate how suitable the different score specifications are to detect DIF
(or lack thereof) a Monte Carlo study has been performed. The \proglang{R} 
system for statistical computing \citep{psychomix:RCore:2.15.2} was used with 
the add-on packages \pkg{psychomix} \citep{psychomix:Frick+Strobl+Leisch:2012} 
and \pkg{clv} \citep{psychomix:Nieweglowski:2009}.


\subsection{Simulation design} \label{sec:design}

<<simPrep, echo = FALSE>>=
## function to generate a design-list for simRaschmix()
generateDesign <- function(nobs = 500, m = 20, weights = NULL,
                           ab = 0, ab.dist = c("fix", "normal"),
                           dif = 2, beta = 1.9, index = 5, coincide = TRUE){
  ## weights
  if (is.null(weights)) weights <- rep(0.25, 4)
  
  ## coincide
  ## can only be set to FALSE if there are differences in both abilities and items
  ## = if either is 0, it has to be TRUE
  if (any(isTRUE(all.equal(c(ab, dif), 0)))) coincide <- TRUE
  
  ## ability
  ab.dist <- match.arg(ab.dist, c("fix", "normal"))
  ab <- c(-ab, ab)
  ab <- if (coincide) rep(ab, each = 2) else rep(ab, times = 2)
  
  ability <- if(ab.dist == "fix"){
    array(c(rbind(ab, 1)), dim = c(1,2,4)) # 1 = rel.frequency in sample()
  } else {
    rbind(ab, 0.5) ## 0.5 = sd for rnorm()
  }

  ## difficulty
  beta <- beta2 <- seq(from = -beta, to = beta, length = m)
  for (i in index){
    beta2[i] <- beta2[i] + dif
    beta2[m-i+1] <- beta2[m-i+1] - dif
  }
  difficulty <- cbind(beta, beta, beta2, beta2)
  
  return(list(nobs = nobs, weights = weights, ability = ability,
              difficulty = difficulty))
  ## NOTE: simRaschmix will return a cluster attribute with 4 different values/classes.
}

stacked_bars <- function(rs, cl = NULL, max = NULL, col = NULL, ...)
{
   if(is.null(max)) max <- max(rs)
   rs <- factor(rs, levels = 0:max)
   if(is.null(cl)) {
     tab <- table(rs)
     names(tab)[diff(-1:max %/% 5) < 1] <- ""
     if(is.null(col)) col <- gray.colors(2)[2]
   } else {
     tab <- table(cl, rs)
     colnames(tab)[diff(-1:max %/% 5) < 1] <- ""
     if(is.null(col)) col <- gray.colors(nrow(tab))
   }
   tab <- prop.table(tab)
   bp <- barplot(tab, space = 0, ...)
}

## colors
mygrays <- gray.colors(2)
myhcl <- psychomix:::qualitative_hcl(3)

## load simulated data
load("scoresim.rda")
scoresim$prop23 <-  1 - scoresim$prop1
@ 

The simulations included in \cite{psychomix:Rost:1990} are the starting point
for developing the design used here. Similar to the original simulation study,
the item parameters represent a test with increasingly difficult items. Here,
20~items are employed with corresponding item parameters $\beta^\mathit{I}$
which follow a sequence from $-1.9$ to 1.9 with increments of 0.2 and hence sum
to zero. 
%
\begin{eqnarray*}
  \beta^\mathit{I} &=& (-1.9, -1.7, \ldots, 1.7, 1.9)^{\top}  \label{eq:simD:betaI}\\
  \beta^\mathit{II} &=& (-1.9, -1.7, \ldots, -1.1 + \Delta, \ldots, 1.1 - \Delta, \ldots, 1.7, 1.9)^{\top}  \label{eq:simD:betaII}
\end{eqnarray*}
%
To introduce DIF, a second set of item parameters $\beta^\mathit{II}$ is 
considered where items 5 and 15 are changed by $\pm \Delta$. This approach is 
similar in spirit to that of \cite{psychomix:Rost:1990} -- who reverses the full
sequence of item parameters to generate DIF -- but allows for gradually changing
from small to large DIF effect sizes. As in \cite{psychomix:Rost:1990}, the 
abilities are drawn from a discrete distribution. For simplicity, they are drawn
with equal proportions from two values only,$-\Theta/2$ and $+\Theta/2$, thus 
creating a sample where half of the subjects have an ability of $-\Theta/2$ and
the other half an abilty of $+\Theta/2$. Such a difference in ability is often 
called impact \citep{psychomix:Ackerman:1992}.

In the simulations below, the DIF effect size $\Delta$ ranges from 0 to 4 in 
steps of 0.2 
%
\begin{equation*}
  \label{eq:simD:Delta}
  \Delta ~\in~ \{0, 0.2, \ldots, 4\}
\end{equation*}
%
while the impact $\Theta$ covers the same range in steps of 0.4:
%
\begin{equation*}
  \label{eq:simD:Theta}
  \Theta ~\in~ \{0, 0.4, \ldots, 4\}.
\end{equation*}
%

Drawing abilities from two normal distributions with means $-\Theta/2$ 
and $+\Theta/2$ leads to qualitatively similar results which are not reported 
here.

\begin{table}[t!]
  \small
  \centering
  \begin{tabular}{rlcccc}
    \hline\noalign{\smallskip}
    \multicolumn{2}{l}{Scenario} & \multicolumn{2}{c}{Latent class~I} &  \multicolumn{2}{c}{Latent class~II} \\
    & & Abilities & Difficulties & Abilities & Difficulties \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    \multicolumn{6}{l}{\em No impact $(\Theta = 0)$}\\ \noalign{\smallskip}
    1 & no DIF $(\Delta = 0)$ & $\{ 0 \}$ & $\beta^\mathit{I}$ & --- & --- \\
    2 & DIF $(\Delta > 0)$ & $\{ 0 \}$ & $\beta^\mathit{I}$ & $\{ 0 \}$ & $\beta^\mathit{II}$ \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    \multicolumn{6}{l}{\em Impact $(\Theta > 0)$}\\ \noalign{\smallskip}
    3 & no DIF $(\Delta = 0)$ & $\{ -\Theta/2, +\Theta/2 \}$ & $\beta^\mathit{I}$ & --- & --- \\
    4 & DIF $(\Delta > 0)$, impact within & $\{ -\Theta/2, +\Theta/2 \}$ & $\beta^\mathit{I}$ & $\{ -\Theta/2, +\Theta/2 \}$ & $\beta^\mathit{II}$ \\
    5 & DIF $(\Delta > 0)$, impact between & $\{ -\Theta/2 \}$ & $\beta^\mathit{I}$ & $\{ +\Theta/2 \}$ & $\beta^\mathit{II}$ \\
   \noalign{\smallskip}\hline
  \end{tabular}
  \caption{Simulation design. The latent-class-specific item parameters $\beta^\mathit{I}$ and 
    $\beta^\mathit{II}$ differ by $\Delta$ for two elements and thus coincide for $\Delta = 0$, 
    leaving only a single latent class.
  \label{tab:simD:design}}
\end{table}

\setkeys{Gin}{width = \textwidth}
\begin{figure}[p!]
\centering
%
<<fig-simD-DIFhomo, echo = FALSE, fig = TRUE, width = 8, height = 2.9>>=
## frame
par(mfrow = c(1,2))
par(mar = c(2, 4, 2, 2) + 0.1) # c(bottom, left, top, right)

## only DIF
des <- generateDesign(ab = 0, dif = 2)
set.seed(1)
dat <- simRaschmix(des)
rs <- rowSums(dat)
cl <- factor(as.numeric(attr(dat, "cluster") > 2) + 1)
ip <- attr(dat, "difficulty")[,2:3]

plot(ip[,1], type = "n", ylab = "Item difficulty", xlab = "")
points(ip[,2], type = "o", pch = 21, col = 1, bg = mygrays[2], lty = 2)
points(ip[,1], pch = 20, col = mygrays[1])

stacked_bars(rs, cl, max = 20, ylab = "Score frequencies")
@ 
%
\caption{Scenario 2. Left: Item difficulties with DIF ($\Delta = 2$). Right: Stacked histogram of unimodal score distribution with homogeneous abilities ($\Theta = 0$).
\label{fig:simD:DIFhomo}}
\end{figure}


\begin{figure}[p!]
\centering
% 
<<fig-simD-noDIFhet, echo = FALSE, fig = TRUE, width = 8, height = 2.9>>=
## frame
par(mfrow = c(1, 2))
par(mar = c(2, 4, 2, 2) + 0.1) 

## only heterogeneous abilities
des <- generateDesign(ab = 1, dif = 0) ## ab = 1 --> impact = 2
set.seed(1)
dat <- simRaschmix(des)
rs <- rowSums(dat)
cl <- factor(as.numeric(attr(dat, "cluster") > 2) + 1)
ip <- attr(dat, "difficulty")[,2:3]

plot(ip[,1], type = "b", pch = 21, bg = mygrays[2], lty = 2,  ylab = "Item difficulty", xlab = "")

stacked_bars(rs, cl = NULL, max = 20, ylab = "Score frequencies", xlab = "")

par(mfrow = c(1, 1))
@ 
%
\caption{Scenario 3. Left: Item difficulties without DIF ($\Delta = 0$). Right: Histogram of bimodal score distribution with impact ($\Theta = 2$).
\label{fig:simD:noDIFhet}}
\end{figure}


\begin{figure}[p!]
\centering
% 
<<fig-simD-DIFhet, echo = FALSE, fig = TRUE, width = 8, height = 2.9>>=
## frame
par(mfrow = c(1,2))
par(mar = c(2, 4, 2, 2) + 0.1)

## designs:
## heterogeneous abilities within DIF groups
des <- generateDesign(ab = 1.0, dif = 2, coincide = FALSE) ## ab = 1 --> impact = 2
set.seed(1)
dat <- simRaschmix(des)
rs <- rowSums(dat)
cl <- factor(as.numeric(attr(dat, "cluster") > 2) + 1)
ip <- attr(dat, "difficulty")[,2:3]

stacked_bars(rs, cl, max = 20, ylab = "Score frequencies", xlab = "")

## heterogeneous abilities between DIF groups
des <- generateDesign(ab = 1.0, dif = 2, coincide = TRUE) ## ab = 1 --> impact = 2
set.seed(1)
dat <- simRaschmix(des)
rs <- rowSums(dat)
cl <- factor(as.numeric(attr(dat, "cluster") > 2) + 1)
ip <- attr(dat, "difficulty")[,2:3]

stacked_bars(rs, cl, max = 20, ylab = "Score frequencies", xlab = "")

par(mfrow = c(1,1))
@ 
%
\caption{Stacked histograms of score distributions for Scenarios~4 (left) and~5
  (right) with DIF ($\Delta = 2$). Left: impact within groups ($\Theta = 2$). 
  Right: impact between groups ($\Theta = 2$). For item difficulties see 
  Figure~\ref{fig:simD:DIFhomo} (left).
  \label{fig:simD:DIFhet}}
\end{figure}

Impact and DIF, or lack thereof, can be combined in several ways.
Table~\ref{tab:simD:design} provides an overview and 
Figures~\ref{fig:simD:DIFhomo}, \ref{fig:simD:noDIFhet}, 
and~\ref{fig:simD:DIFhet} show illustrations:
%
\begin{itemize}
  \item If the simulation parameter~$\Delta$ for the DIF effect size is set to 
    zero, both sets of item parameters, $\beta^\mathit{I}$ and $\beta^\mathit{II}$,
    are identical and no DIF is present. Since CML is employed, model selection
    and parameter estimation is typically expected to be independent of whether
    or not an impact is present (Scenario~1 and~3 in 
    Table~\ref{tab:simD:design}).
    
  \item If $\Delta > 0$, the item parameter set $\beta^\mathit{II}$ is 
    different from $\beta^\mathit{I}$. Hence, there is DIF and two latent classes
    exist (Scenarios~2, 4, and~5). Both classes are chosen to be of equal size 
    in this case. For an illustration see the left panel of
    Figure~\ref{fig:simD:DIFhomo}.

  \item If the simulation parameter $\Theta$ for the impact is set to zero, 
    abilities are homogeneous across all subjects (Scenarios~1 and~2) and the 
    resulting score distribution is unimodal. For an illustration see the right
    panel of Figure~\ref{fig:simD:DIFhomo}. The histogram is shaded in light and
    dark gray for the two DIF groups present in this example from Scenario 2 and
    thus to be read like a \dquote{stacked histogram}.
    
  \item If $\Theta > 0$, subject abilities are sampled from 
    $\{-\Theta/2, +\Theta/2\}$ with equal weights, thus generating impact. When
    no DIF is included (Scenario~3), the resulting score distribution moves from
    being unimodal to being bimodal with increasing $\Theta$. For the
    illustration in Figure~\ref{fig:simD:noDIFhet} only a medium gray is used to
    shade the histogram as no DIF groups are present. However, if there is 
    DIF~(i.e., $\Delta > 0$), two combinations of DIF with impact are 
    considered: Impact can occur within each DIF group (Scenario~4) or between 
    DIF groups (Scenario~5). Illustrations of the resulting score distributions
    can be found in Figure~\ref{fig:simD:DIFhet}. 
\end{itemize}
%
Note that Scenario~1 is a special case of Scenario~2 where $\Delta$ is 
reduced to zero as well as a special case of Scenario~3 where $\Theta$ is 
reduced to zero. Therefore, in the following, Scenario~1 is not inspected 
separately but included in both the setting of
\emph{No impact with DIF} (Scenario~2) and the setting of
\emph{Impact without DIF} (Scenario~3) as a reference point. 
Similarly, Scenarios~4 and~5 both can be reduced to Scenario~3 if $\Delta$ is 
set to zero. It is therefore also included in both the setting of
\emph{Impact within DIF groups} (Scenario~4) and the setting of
\emph{Impact between DIF groups} (Scenario~5) as a reference point.

Also note that the Scenarios~3 and~4 essentially correspond to the designs~1 
and~2 of \cite{psychomix:Rost:1990}.

For each considered combination of $\Delta$ and $\Theta$, 500 datasets of 500 
observations each are generated. Observations with raw scores of 0 or $m$ are
removed from the dataset as they do not contribute to the estimation of the
Rasch mixture model \citep{psychomix:Rost:1990}. For each dataset, Rasch mixture
models for each of the saturated, mean-variance, and restricted score 
specifications are fitted for $K = 1, 2, 3$.


\subsection{Type I error and power in DIF detection} \label{sec:DIFdetection}

The main objective here is to determine how suitable a Rasch mixture model, 
with various choices for the score model, is to recognize DIF or the lack 
thereof. 

For each dataset and type of score model, models with $K = 1, 2, 3$ latent 
classes are fitted and the $\hat K$ associated with the minimum BIC is selected.
Choosing one latent class ($\hat K = 1$) then corresponds to assuming 
measurement invariance while choosing more than one latent class ($\hat K > 1$)
corresponds to finding DIF in at least one item of the test. The empirical 
proportion among the 500~datasets with $\hat K > 1$ then essentially corresponds
to the power of DIF detection if $\Delta > 0$ (and thus two true latent classes)
and to the associated type I error if $\Delta = 0$ (and thus only one true 
latent class).


\subsubsection{Scenario 2: No impact with DIF}

This scenario is investigated as it is a case of DIF that should be fairly 
simple to detect. There is no impact as abilities are homogeneous across all 
subjects so the only latent structure to detect is the group membership based on
the two item profiles. This latent structure is made increasingly easy to detect
by increasing the difference between the item difficulties for both latent 
groups. In the graphical representation of the item parameters (left panel of
Figure~\ref{fig:simD:DIFhomo}) this corresponds to enlarging the spikes in the
item profile. 

Figure~\ref{fig:simR:DIFhomo} shows how the rate of choosing a model with more 
than one latent class ($\hat K > 1$) increases along with the DIF effect 
size~$\Delta$. At $\Delta = 0$ this corresponds to the type I error rate of DIF
detection which is very close to zero for all three score distributions. With 
increasing $\Delta > 0$ the rate corresponds to the power of DIF detection and 
increases as well. For low values of $\Delta$ none of the three models is able 
to pick up the DIF but at around $\Delta = 3$ the two more parsimonious versions
of the Rasch mixture model (with mean-variance and restricted score 
distribution) start to have increasing power which almost approaches $1$ at 
$\Delta = 4$. Not surprisingly, the restricted score specification performs 
somewhat better because in fact the raw score distributions do not differ
between the two latent classes. The saturated score model, however, has almost
no power over the range of $\Delta$ considered. The reason is that it requires 
18~additional score parameters for an additional latent class which is 
\dquote{too costly} in terms of BIC. Hence, $\hat K = 1$ is chosen for almost 
all Rasch mixture models using a saturated score distribution.

\emph{Brief summary:} The mean-variance and restricted model have a higher power
than the saturated model in the absense of impact.


\setkeys{Gin}{width = 0.65\textwidth}
\begin{figure}[t!]
\centering
%  
<<fig-simR-DIFhomo, echo = FALSE, fig = TRUE, width = 6.5, height = 4.5>>=
par(mar = c(4, 4, 2, 2) + 0.1)

plot(prop23 ~ delta, data = scoresim, subset = theta == 0 & scores == "saturated", 
     ylim = c(0,1), type = "b", 
     xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
     ylab = "Choose more than 1 latent class", col = myhcl[3], lty = 3, pch = 3)
lines(prop23 ~ delta, data = scoresim, subset = theta == 0 & scores == "meanvar", 
     type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(prop23 ~ delta, data = scoresim, subset = theta == 0 & scores == "restricted", 
     type = "b", col = myhcl[1], lty = 2, pch = 6)
legend("topleft", legend = c("saturated", "mean-variance", "restricted"), 
       col = myhcl[3:1], lty = c(3,1,2), pch = c(3,1,6), bty = "n")

par(mar = c(5, 4, 4, 2) + 0.1)
@
%
\caption{Rate of choosing a model with $\hat K > 1$ latent classes for data from
Scenario~2 (DIF without impact, i.e., $\Theta = 0$).
\label{fig:simR:DIFhomo}}
\end{figure}


\subsubsection{Scenario 3: Impact without DIF}

Preferably, a Rasch mixture model should not only detect latent classes if the
assumption of measurement invariance is violated but it should also indicate a
lack of latent structure if indeed the assumption holds. In this scenario, the
subjects all stem from the same class, meaning each item is of the same
difficulty for every subject. However, subject abilities are simulated with 
impact resulting in a bimodal score distribution as illustrated in 
Figure~\ref{fig:simD:noDIFhet}.

Here, the rate of choosing more than one latent class can be interpreted as a 
false discovery or false alarm rate (Figure~\ref{fig:simR:noDIFhet}). In the 
setting of a test this would correspond to the type I error of the test. The 
restricted score model is invariant against any latent structure in the score 
distribution and thus almost always suggests $\hat K = 1$ latent class based on
the DIF-free item difficulties. The saturated model also picks $\hat K = 1$ in
most of the simulation. This might be due to its general reluctance to choose
more than one latent class as illustrated in Figure~\ref{fig:simR:DIFhomo} or
the circumstance that it can assume any shape (including bimodal patterns).
However, the mean-variance score distribution can only model unimodal or 
U-shaped distributions as mentioned above. Hence, with increasing impact and
thus increasingly well-separated modes in the score distribution, the Rasch 
mixture model with this score specification often suggests $\hat K > 1$ latent 
classes. Note, however, that these latent classes do not represent the DIF 
groups (as there are none) but rather groups of subjects with high vs.\ low 
abilities. While this may be acceptable (albeit unnecessarily complex) from a 
statistical mixture modeling perspective, it is misleading from a psychometric 
point of view if the aim is DIF detection. Only one Rasch model needs to be 
estimated for this type of data, consistent item parameter estimates can be 
obtained via CML and all observations can be scaled in the same way.

\emph{Brief summary:} If measurement invariance holds but ability differences 
are present, the mean-variance model exhibits a high false alarm rate while the
saturated and restricted model are not affected.


\begin{figure}[t!]
\centering
%  
<<fig-simR-noDIFhet, echo = FALSE, fig = TRUE, width = 6.5, height = 4.5>>=
par(mar = c(4, 4, 2, 2) + 0.1)

plot(prop23 ~ theta, data = scoresim, subset = delta == 0 & scores == "saturated", 
     ylim = c(0,1), type = "b", 
     xlab = expression(paste("Impact (", Theta, ")", sep = "")),
     ylab = "Choose more than 1 latent class", col = myhcl[3], lty = 3, pch = 3)
lines(prop23 ~ theta, data = scoresim, subset = delta == 0 & scores == "meanvar", 
     type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(prop23 ~ theta, data = scoresim, subset = delta == 0 & scores == "restricted", 
     type = "b", col = myhcl[1], lty = 2, pch = 6)
legend("topleft", legend = c("saturated", "mean-variance", "restricted"), 
       col = myhcl[3:1], lty = c(3,1,2), pch = c(3,1,6), bty = "n")

par(mar = c(5, 4, 4, 2) + 0.1)
@
%
\caption{Rate of choosing a model with $\hat K > 1$ latent classes for data from
Scenario~3 (impact without DIF, i.e., $\Delta = 0$).
\label{fig:simR:noDIFhet}}
\end{figure}


\subsubsection{Scenario 4: Impact within DIF groups}

In this scenario, there is DIF (and thus two true latent classes) if 
$\Delta > 0$. Again, Scenario~3 with $\Delta = 0$ (and thus without DIF) is 
included as a reference point. However, unlike in Scenario~2 the abilities 
within the latent classes are not homogeneous but impact exists within each DIF
group. Nonetheless, the score distribution is the same across both latent 
classes (illustrated in the left panel of Figure~\ref{fig:simD:DIFhet}).

Figure~\ref{fig:simR:DIFhetWithin} again shows the rate of choosing $\hat K > 1$
for increasing DIF effect size $\Delta$ for different levels of impact 
$\Theta = 0.4, 2.0, 3.6$. If impact is small (left panel with $\Theta = 0.4$), 
the rates are very similar to the case of completely homogeneous abilities 
without impact (Figure~\ref{fig:simR:DIFhomo} with $\Theta = 0$). While the 
rates for the restricted and the saturated score model do not change 
substantially for an increased impact ($\Theta = 2.0$ and $3.6$), the 
mean-variance model is influenced by this change in ability differences. While 
power is increased over the whole range of $\Delta$, the type I error (or false
alarm rate) at $\Delta = 0$ is increased to the same extent. Moreover, the 
detection rate only increases noticeably beyond the initial type I error rate 
at around $\Delta = 3$, i.e., the same DIF effect size at which the restricted 
and mean-variance specifications have power given homogeneous abilities without
impact. Thus, given rather high impact ($\Theta = 3.6$) the power is not driven
by the DIF detection but rather the model's tendency to assign subjects with
high vs.\ low abilities into different groups (as already seen in 
Figure~\ref{fig:simR:noDIFhet}).

\setkeys{Gin}{width = \textwidth}
\begin{figure}[t!]
\centering
%
<<fig-simR-DIFhetWithin, echo = FALSE, fig = TRUE, width = 7, height = 2.5>>=
par(mfrow = c(1, 3))
layout(matrix(c(rep(1,5), rep(2,4), rep(3,5)), nrow = 1, byrow = TRUE))

## impact = 0.4
par(mar = c(5, 4, 4, 0.5) + 0.1)
plot(prop23 ~ delta, data = scoresim, 
  subset = theta == 0.4 & scores == "saturated" & (scenario == 4 | delta == 0), 
  main = expression(paste("Impact ", Theta, " = 0.4", sep = "")),
  ylim = c(0,1), type = "b", 
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "Choose more than 1 latent class", col = myhcl[3], lty = 3, pch = 3)
lines(prop23 ~ delta, 
  data = scoresim, subset = theta == 0.4 & scores == "meanvar" & (scenario == 4 | delta == 0), 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 0.4 & scores == "restricted" & (scenario == 4 | delta == 0),
  type = "b", col = myhcl[1], lty = 2, pch = 6)
legend("topleft", legend = c("saturated", "mean-variance", "restricted"), 
  col = myhcl[3:1], lty = c(3,1,2), pch = c(3,1,6), bty = "n")
                
## impact = 2.0
par(mar = c(5, 0.5, 4, 0.5) + 0.1) # c(bottom, left, top, right)
plot(prop23 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "saturated" & (scenario == 4 | delta == 0), 
  main = expression(paste("Impact ", Theta, " = 2.0", sep = "")),
  ylim = c(0,1), type = "b", axes = FALSE,
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "", col = myhcl[3], lty = 3, pch = 3)
box(); axis(1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "meanvar" & (scenario == 4 | delta == 0), 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "restricted" & (scenario == 4 | delta == 0),
  type = "b", col = myhcl[1], lty = 2, pch = 6)

## impact = 3.6
par(mar = c(5, 0.5, 4, 4) + 0.1) # c(bottom, left, top, right)
plot(prop23 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "saturated" & (scenario == 4 | delta == 0), 
  main = expression(paste("Impact ", Theta, " = 3.6", sep = "")),
  ylim = c(0,1), type = "b", axes = FALSE,
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "", col = myhcl[3], lty = 3, pch = 3)
box(); axis(1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "meanvar" & (scenario == 4 | delta == 0), 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "restricted" & (scenario == 4 | delta == 0),
  type = "b", col = myhcl[1], lty = 2, pch = 6)

par(mar = c(5, 4, 4, 2) + 0.1, mfrow = c(1,1)) ## restore default
@
%
\caption{Rate of choosing a model with $\hat K > 1$ latent classes for data from
  Scenario~4 (impact within DIF groups).
  \label{fig:simR:DIFhetWithin}}
\end{figure}

As Rasch mixture models with $K = 1, 2, 3$ classes are considered, selecting
$\hat K > 1$ classes can either mean selecting the correct number of $K = 2$
or overselecting $K = 3$ classes. For the saturated and restricted 
specifications overselection is rare (occurring with rates of less than 5\% or 
less than 1\%, respectively). However, similar to Scenario~3 overselection is 
not rare for the mean-variance specification. Figure~\ref{fig:simR:prop23} 
depicts the rates of selecting $\hat K = 2$ and $\hat K = 3$ classes, 
respectively, for increasing $\Delta$ at $\Theta = 3.6$ (hollow symbols for 
Scenario~4). The rate for overselection ($\hat K = 3$) is already at around 
30\% for low values of $\Delta$ and even increases somewhat further starting
from around $\Delta = 3$.

\emph{Brief summary:} If impact is simulated within DIF groups, the 
mean-variance model has higher power than the saturated and resticted models. 
However, the latent classes estimated by the mean-variance model are mostly 
based on ability differences when the DIF effect size is low. If the DIF effect 
size is high, the mean-variance model tends to overestimate the number of 
classes.


\setkeys{Gin}{width = 0.65\textwidth}
\begin{figure}[t!]
\centering
%  
<<fig-simR-prop23, echo = FALSE, fig = TRUE, width = 6.5, height = 4.5>>=
par(mar = c(4, 4, 4, 2) + 0.1)

plot(prop3 ~ delta, data = scoresim,
  subset = theta == 3.6 & scores == "meanvar" & (scenario == 4 | delta == 0),
  type = "b", ylim = c(0, 1), col = myhcl[2], pch = 1,
  main = expression(paste("Impact ", Theta, " = 3.6", sep = "")),
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "Selection proportions")
lines(prop3 ~ delta, data = scoresim,
  subset = theta == 3.6 & scores == "meanvar" & (scenario == 5 | delta == 0),
  type = "b", col = myhcl[2], pch = 16)

lines(prop2 ~ delta, data = scoresim,
  subset = theta == 3.6 & scores == "meanvar" & (scenario == 4 | delta == 0),
  type = "b", col = myhcl[2], pch = 2)
lines(prop2 ~ delta, data = scoresim,
  subset = theta == 3.6 & scores == "meanvar" & (scenario == 5 | delta == 0),
  type = "b", col = myhcl[2], pch = 17)

legend("topleft", 
       legend = c(expression(paste(hat(K), " = 2 - Sc 4", sep = "")),
           expression(paste(hat(K), " = 3 - Sc 4", sep = "")),
           expression(paste(hat(K), " = 2 - Sc 5", sep = "")),
           expression(paste(hat(K), " = 3 - Sc 5", sep = ""))),
           #"2 - Sc 4", "3 - Sc 4", "2 - Sc 5", "3 - Sc 5"), 
  col = myhcl[2], lty = 1, pch = c(2, 1, 17, 16), bty = "n")

par(mar = c(5, 4, 4, 2) + 0.1)
@
%
\caption{Rates of choosing the correct number of classes ($\hat K = 2$) or 
  overselecting the number of classes ($\hat K = 3$) for the Rasch mixture model
  with mean-variance score specification in Scenarios~4 (hollow, impact within 
  DIF groups) and~5 (solid, impact between DIF groups).}
\label{fig:simR:prop23}
\end{figure}
\setkeys{Gin}{width = \textwidth}


\subsubsection{Scenario 5: Impact between DIF groups}

In Scenario~5, there is also DIF (i.e., $\Delta > 0$) and impact.
However, in contrast to Scenario~4 impact exists between the DIF groups but
not within (see the right panel of Figure~\ref{fig:simD:DIFhet}). Furthermore,
Scenario~3 is included also here as the reference point without DIF 
($\Delta = 0$).

Again, small ability differences do not strongly influence the rate of choosing 
more than one latent class (compare left panel of 
Figure~\ref{fig:simR:DIFhetBetween} and Figure~\ref{fig:simR:DIFhomo}). Both 
mean-variance and restricted specification have comparable power for DIF 
detection starting from around $\Delta = 3$ while the saturated specification 
has very lower power. As impact increases (middle and right panels of 
Figure~\ref{fig:simR:DIFhetBetween}), the power of all models increases as well
because the ability differences contain information about the DIF groups: 
separating subjects with low and high abilities also separates the two DIF 
groups (not separating subjects within each DIF group as in the previous 
setting). However, for the mean-variance model this increased power is again 
coupled with a drastically increased false alarm rate at $\Delta = 0$. The 
restricted score model, on the other hand, is invariant to latent structure in 
the score distribution and thus performs similarly as in previous DIF scenarios,
suggesting more than one latent class past a certain threshold of DIF intensity,
albeit this threshold being a bit lower than in the case of impact within DIF 
groups (around $\Delta = 2$). The saturated model detects more than one latent 
class at a similarly low or lower rate than the other two models regardless of 
the level of impact.

\begin{figure}[t!]
\centering
% 
<<fig-simR-DIFhetBetween, echo = FALSE, fig = TRUE, width = 7, height = 2.5>>=
par(mfrow = c(1, 3))
layout(matrix(c(rep(1,5), rep(2,4), rep(3,5)), nrow = 1, byrow = TRUE))

## impact = 0.4
par(mar = c(5, 4, 4, 0.5) + 0.1)
plot(prop23 ~ delta, data = scoresim, 
  subset = theta == 0.4 & scores == "saturated" & (scenario == 5 | delta == 0), 
  main = expression(paste("Impact ", Theta, " = 0.4", sep = "")),
  ylim = c(0,1), type = "b", 
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "Choose more than 1 latent class", col = myhcl[3], lty = 3, pch = 3)
lines(prop23 ~ delta, 
  data = scoresim, subset = theta == 0.4 & scores == "meanvar" & (scenario == 5 | delta == 0), 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 0.4 & scores == "restricted" & (scenario == 5 | delta == 0),
  type = "b", col = myhcl[1], lty = 2, pch = 6)
legend("topleft", legend = c("saturated", "mean-variance", "restricted"), 
  col = myhcl[3:1], lty = c(3,1,2), pch = c(3,1,6), bty = "n")

## impact = 2.0
par(mar = c(5, 0.5, 4, 0.5) + 0.1) # c(bottom, left, top, right)
plot(prop23 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "saturated" & (scenario == 5 | delta == 0), 
  main = expression(paste("Impact ", Theta, " = 2.0", sep = "")),
  ylim = c(0,1), type = "b", axes = FALSE,
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "", col = myhcl[3], lty = 3, pch = 3)
box(); axis(1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "meanvar" & (scenario == 5 | delta == 0), 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "restricted" & (scenario == 5 | delta == 0),
  type = "b", col = myhcl[1], lty = 2, pch = 6)

## impact = 3.6
par(mar = c(5, 0.5, 4, 4) + 0.1) # c(bottom, left, top, right)
plot(prop23 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "saturated" & (scenario == 5 | delta == 0), 
  main = expression(paste("Impact ", Theta, " = 3.6", sep = "")),
  ylim = c(0,1), type = "b", axes = FALSE,
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "", col = myhcl[3], lty = 3, pch = 3)
box(); axis(1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "meanvar" & (scenario == 5 | delta == 0), 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(prop23 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "restricted" & (scenario == 5 | delta == 0),
  type = "b", col = myhcl[1], lty = 2, pch = 6)

par(mar = c(5, 4, 4, 2) + 0.1, mfrow = c(1,1)) ## restore default
@
%
\caption{Rate of choosing a model with $\hat K > 1$ latent classes for data from
  Scenario~5 (impact between DIF groups).
  \label{fig:simR:DIFhetBetween}}
\end{figure}

Finally, the potential issue of overselection can be considered again.
Figure~\ref{fig:simR:prop23} (solid symbols) shows that this problem disappears
for the mean-variance specification if both DIF effect size $\Delta$ and impact
are large \emph{and} coincide. For the restricted model overselection is again 
very rare throughout (occurring in less than 1\% of all cases) while the 
saturated model overselects in up to 25\% of the datasets.

\emph{Brief summary:} If abilities differ between DIF groups, the mean-variance 
model detects the violation of measurement invariance for smaller DIF effect 
sizes than the saturated and restricted model. While the mean-variance model 
does not overselect the number of components in this scenario, the high power 
is connected to a high false alarm rate when no DIF is present but impact is 
high. This does not affect the other two score models.


\subsection{Quality of estimation} \label{sec:quality}

Once the number of latent classes is established/estimated, it is of interest 
how well the estimated model fits the data. In the context of Rasch mixture 
models with different score distributions, the posterior probabilities 
$\hat p_{ik}$ (Equation~\ref{eq:RMM:Estep:hg}) are crucial as the estimation of 
the item parameters depends on the score distribution only through these. Thus,
if the $\hat p_{ik}$ were the same for all three score specifications, the 
estimated item difficulties were the same as well. Hence, it needs to be 
assessed how close the estimated posterior probabilities are to the true latent 
classes in the data. If the similarity between these is high, CML estimation of 
the item parameters within the classes will also yield better results.

This is a standard task in the field of cluster analysis and we adopt the widely
used Rand index \citep{psychomix:Rand:1971} here: Each observation is assigned
to the latent class for which its posterior probability is highest and then 
pairs of observations are considered. Each pair can either be in the same class
in both the true and the estimated classification, in different classes for both
classifications or it can be in the same class for one but not the other
classification. The Rand index is the proportion of pairs for which both 
classifications agree. Thus, it can assume values between 0 and 1, indicating 
total dissimilarity and similarity, respectively.

\setkeys{Gin}{width = \textwidth}
\begin{figure}[t!]
\centering
%
<<fig-simR-DIFhetWithinRand2, echo = FALSE, fig = TRUE, width = 7, height = 2.5>>=
par(mfrow = c(1, 3))
layout(matrix(c(rep(1,5), rep(2,4), rep(3,5)), nrow = 1, byrow = TRUE))

#par(mar = c(5, 4, 4, 6) + 0.1) # c(bottom, left, top, right)
par(mar = c(5, 4, 4, 0.5) + 0.1)
plot(rand2 ~ delta, data = scoresim, 
  subset = theta == 0.4 & scores == "saturated" & scenario == 4 & delta > 0, 
  main = expression(paste("Impact ", Theta, " = 0.4", sep = "")),
  ylim = c(0.5,1), type = "b", 
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "Rand index", col = myhcl[3], lty = 3, pch = 3)
lines(rand2 ~ delta, 
  data = scoresim, subset = theta == 0.4 & scores == "meanvar" & scenario == 4 & delta > 0, 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 0.4 & scores == "restricted" & scenario == 4 & delta > 0,
  type = "b", col = myhcl[1], lty = 2, pch = 6)
legend("topleft", legend = c("saturated", "mean-variance", "restricted"), 
  col = myhcl[3:1], lty = c(3,1,2), pch = c(3,1,6), bty = "n")
                                        
par(mar = c(5, 0.5, 4, 0.5) + 0.1) # c(bottom, left, top, right)
plot(rand2 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "saturated" & scenario == 4 & delta > 0, 
  main = expression(paste("Impact ", Theta, " = 2.0", sep = "")),
  ylim = c(0.5,1), type = "b", axes = FALSE,
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "", col = myhcl[3], lty = 3, pch = 3)
box(); axis(1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "meanvar" & scenario == 4 & delta > 0, 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "restricted" & scenario == 4 & delta > 0,
  type = "b", col = myhcl[1], lty = 2, pch = 6)

par(mar = c(5, 0.5, 4, 4) + 0.1) # c(bottom, left, top, right)
plot(rand2 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "saturated" & scenario == 4 & delta > 0, 
  main = expression(paste("Impact ", Theta, " = 3.6", sep = "")),
  ylim = c(0.5,1), type = "b", axes = FALSE,
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "", col = myhcl[3], lty = 3, pch = 3)
box(); axis(1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "meanvar" & scenario == 4 & delta > 0, 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "restricted" & scenario == 4 & delta > 0,
  type = "b", col = myhcl[1], lty = 2, pch = 6)

par(mar = c(5, 4, 4, 2) + 0.1, mfrow = c(1,1)) ## restore default
@
%
\caption{Average Rand index for models with $K = 2$ latent classes for data 
  from Scenario~4 (impact within DIF groups).
  \label{fig:simR:DIFhetWithinRand2}}
\end{figure}

In the following, the Rand index for models with the true number of
$K = 2$ latent classes in Scenarios~4 and~5 (with DIF) is considered. Thus,
the question of DIF detection (or model selection) is not investigated again
but only the quality of latent class recovery (assuming the number of classes
$K$ to be known or correctly selected). Figure~\ref{fig:simR:DIFhetWithinRand2}
depicts the average Rand index for data from Scenario~4 (impact within DIF 
groups). Here, all three score specifications find similarly well matching 
classifications, while the Rand index generally decreases with increasing
impact (left to right panel). In particular, while the mean-variance score model
has problems finding the \emph{correct number} of latent classes in this 
scenario, it only performs slightly worse than the other two specifications in 
determining the \emph{correct classes} if the number were known. Similarly, if
it is provided with the correct number of classes, the saturated model also 
identifies the correct classes equally well compared to the other models -- 
while it hardly ever chooses the correct number of classes in the first place.

However, in Scenario~5 where the score distribution contains information about
the DIF groups, the three score specifications perform very differently as
Figure~\ref{fig:simR:DIFhetBetweenRand2} shows. Given the correct number of 
classes, the mean-variance model is most suitable to uncover the true latent 
classes, yielding Rand indices close to 1 if both DIF effect size and impact are
large. The saturated specification follows a similar pattern albeit with poorer 
results. However, the classifications obtained from the restricted score 
specification do not match the true groups well in this scenario. The reason is
that the restricted score model is partially misspecified as the score 
distributions differ substantially across DIF groups.

\begin{figure}[b!]
\centering
% 
<<fig-simR-DIFhetBetweenRand2, echo = FALSE, fig = TRUE, width = 7, height = 2.5>>=
par(mfrow = c(1, 3))
layout(matrix(c(rep(1,5), rep(2,4), rep(3,5)), nrow = 1, byrow = TRUE))

#par(mar = c(5, 4, 4, 6) + 0.1) # c(bottom, left, top, right)
par(mar = c(5, 4, 4, 0.5) + 0.1)
plot(rand2 ~ delta, data = scoresim, 
  subset = theta == 0.4 & scores == "saturated" & scenario == 5 & delta > 0, 
  main = expression(paste("Impact ", Theta, " = 0.4", sep = "")),
  ylim = c(0.5,1), type = "b", 
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "Rand index", col = myhcl[3], lty = 3, pch = 3)
lines(rand2 ~ delta, 
  data = scoresim, subset = theta == 0.4 & scores == "meanvar" & scenario == 5 & delta > 0, 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 0.4 & scores == "restricted" & scenario == 5 & delta > 0,
  type = "b", col = myhcl[1], lty = 2, pch = 6)
legend("topleft", legend = c("saturated", "mean-variance", "restricted"), 
  col = myhcl[3:1], lty = c(3,1,2), pch = c(3,1,6), bty = "n")
                                        
par(mar = c(5, 0.5, 4, 0.5) + 0.1) # c(bottom, left, top, right)
plot(rand2 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "saturated" & scenario == 5 & delta > 0, 
  main = expression(paste("Impact ", Theta, " = 2.0", sep = "")),
  ylim = c(0.5,1), type = "b", axes = FALSE,
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "", col = myhcl[3], lty = 3, pch = 3)
box(); axis(1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "meanvar" & scenario == 5 & delta > 0, 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 2.0 & scores == "restricted" & scenario == 5 & delta > 0,
  type = "b", col = myhcl[1], lty = 2, pch = 6)

par(mar = c(5, 0.5, 4, 4) + 0.1) # c(bottom, left, top, right)
plot(rand2 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "saturated" & scenario == 5 & delta > 0, 
  main = expression(paste("Impact ", Theta, " = 3.6", sep = "")),
  ylim = c(0.5,1), type = "b", axes = FALSE,
  xlab = expression(paste("DIF effect size (", Delta, ")", sep = "")),  
  ylab = "", col = myhcl[3], lty = 3, pch = 3)
box(); axis(1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "meanvar" & scenario == 5 & delta > 0, 
  type = "b", col = myhcl[2], lty = 1, pch = 1)
lines(rand2 ~ delta, data = scoresim, 
  subset = theta == 3.6 & scores == "restricted" & scenario == 5 & delta > 0,
  type = "b", col = myhcl[1], lty = 2, pch = 6)

par(mar = c(5, 4, 4, 2) + 0.1, mfrow = c(1,1)) ## restore default
@
%
\caption{Average Rand index for models with $K = 2$ latent classes for data 
  from Scenario~5 (impact between DIF groups).
  \label{fig:simR:DIFhetBetweenRand2}}
\end{figure}


\subsection{Summary and implications for practical use}\label{sec:discussion}

Given various combinations of DIF and ability impact, the score models are 
differently suitable for the two tasks discussed here -- DIF detection and 
estimation of item parameters in subgroups. Starting with a summary of the 
results for DIF detection: 

%
\begin{itemize}
  \item The saturated score model has much lower power than the other two 
    specifications, i.e., violation of measurement invariance remains too often
    undetected.
  \item The mean-variance model has much better power. However, if impact is 
    present in the abilities, this specification has highly inflated false alarm
    rates. Hence, if the mean-variance model selects more than one latent class
    it is unclear whether this is due to DIF or just varying subject abilities.
    Thus, measurement invariance might still hold even if more than one latent
    class is detected.
  \item The restricted score model also has high power, comparable to the 
    mean-variance model if abilities are rather homogeneous. But unlike the 
    mean-variance specification, its type I error rate is not distorted by 
    impact. Its performance is not influenced by the ability distribution and 
    detecting more than one latent class reliably indicates DIF, i.e., a 
    violation of measurement invariance.
\end{itemize}
%
Hence, if the Rasch mixture model is employed for assessing measurement 
invariance or detecting DIF, then the restricted score specification appears to 
be most robust. Thus, the selection of the number of latent classes should be 
based on this specification.

Given the correct number of classes, the different score models are all 
similarly suitable to detect the true classification if ability impact does not 
contain any additional information about the DIF groups. However, if ability 
impact is highly correlated with DIF groups in the data, this information can be
exploited by the unrestricted specifications while it distracts the restricted 
model.

Thus, while the selection of the number of latent classes should be based only
on the restricted score specification, the unrestricted mean-variance and
saturated specifications might still prove useful for estimating the Rasch
mixture model (after $\hat K$ has been selected). 

We therefore recommend a two step approach for DIF detection via a Rasch 
mixture model. First, the number of latent classes is determined via the 
restricted score model. Second, if furthermore the estimation of the item 
difficulties is of interest, the full selection of score models can the be 
utilized. While the likelihood ratio test is not suitable to test for the number
of latent classes, it can be used to establish the best fitting score model, 
given the number of latent classes. If this approach is applied to the full 
range of score models (saturated and mean-variance, both unrestricted and 
restricted), the nesting structure of the models needs to be kept in mind. 


\section{Empirical application: Verbal aggression}\label{sec:application}

We illustrate this approach on a dataset on verbal aggression 
\citep{psychomix:Boeck+Wilson:2004}. Participants are presented with 
one of two potentially frustrating situations (S1 and S2):
\vspace{0.5cm}
%
\begin{itemize}
  \item S1: A bus fails to stop for me.
  \item S2: I miss a train because a clerk gave me faulty information.
\end{itemize}
%
and a verbally aggressive response (cursing, scolding, shouting). 
Combining each situation and response with either \dquote{I want to} or 
\dquote{I do} leads to the following items:
%
<<VerbalData, echo=FALSE>>=
data("VerbalAggression", package = "psychotools")
VerbalAggression$resp2 <- VerbalAggression$resp2[, 1:12]
va12 <- subset(VerbalAggression, rowSums(resp2) > 0 & rowSums(resp2) < 12)
items <- colnames(va12$resp2)
@ 
%
%
\begin{table*}[h]
  \centering
  \begin{tabular}{llllll}
    \Sexpr{items[1]} & \Sexpr{items[2]} & \Sexpr{items[3]} & \Sexpr{items[4]} & \Sexpr{items[5]} & \Sexpr{items[6]} \\
    \Sexpr{items[7]} & \Sexpr{items[8]} & \Sexpr{items[9]} & \Sexpr{items[10]} & \Sexpr{items[11]} & \Sexpr{items[12]}
  \end{tabular}
\end{table*}
%

First, we determine the number of latent classes $K$ using the BIC for the
Rasch mixture model with a restricted score specification:
%
<<VerbalResFit0, echo = FALSE, eval = FALSE>>=
set.seed(403)
mvR <- raschmix(resp2 ~ 1, data = va12, k = 1:4, scores = "meanvar",
  restricted = TRUE)
@
<<VerbalResFit, echo = FALSE, results = hide>>=
if(cache & file.exists("va12_mvR.rda")){
    load("va12_mvR.rda")
} else {
<<VerbalResFit0>>
    if(cache){
        save(mvR, file = "va12_mvR.rda")
    } else {
        if(file.exists("va12_mvR.rda")) file.remove("va12_mvR.rda")
    }
}
@ 
<<VerbalLC, echo = FALSE>>=
mvRbic <- round(BIC(mvR), digits = 1)
mvR3 <- getModel(mvR, which = "BIC")
@ 
% 
%
\begin{table*}[h]
  \centering
  \begin{tabular}{lrrrr}
    Classes & 1 & 2 & 3 & 4 \\
    BIC & \Sexpr{mvRbic[1]} & \Sexpr{mvRbic[2]} & \Sexpr{mvRbic[3]} & \Sexpr{mvRbic[4]}
  \end{tabular}
\end{table*}
%

Thus, $\hat K = 3$ latent classes are selected. Given this selection of $K$, 
four different models are conceivable: either using a restricted or unrestricted
score model, and either using a saturated or mean-variance specification.
Note that the models with restricted saturated score distribution and restricted
mean-variance score distribution lead to identical item parameter estimates.
However, it is still of interest to fit them separately because each of the
restricted specifications is nested within the corresponding unrestricted
specification. Furthermore, the mean-variance distribution is nested within
the saturated distribution.
%
<<VerbalScoresFit0, echo = FALSE, eval = FALSE>>=
## fit all possible score models
sat3 <- raschmix(resp2 ~ 1, data = va12, k = 3, scores = "saturated")
satR3 <- raschmix(resp2 ~ 1, data = va12, k = 3, scores = "saturated", 
                  restricted = TRUE)
mv3 <- raschmix(resp2 ~ 1, data = va12, k = 3, scores = "meanvar")
@ 
<<VerbalScoresFit, echo = FALSE, results = hide>>=
if(cache & file.exists("va12_m3.rda")){
    load("va12_m3.rda")
} else {
<<VerbalScoresFit0>>
    if(cache){
        save(sat3, satR3, mv3, file = "va12_m3.rda")
    } else {
        if(file.exists("va12_m3.rda")) file.remove("va12_m3.rda")
    }
}
@ 
%
%
<<VerbalLRtest, echo = FALSE, results = hide>>=
## check which score model fits best
library("lmtest")
lrtest(sat3, mv3, mvR3)
lrtest(sat3, satR3, mvR3)
clsizes <- table(clusters(mvR3))
@ 

As $K = 3$ is identical for all of these four models, standard likelihood ratio
tests can be used for comparing all nested models with each other. The results 
for the verbal aggression data are shown in Figure~\ref{fig:verbalTest}. This 
shows that only the likelihood ratio test for restricted vs.\ unrestricted 
saturated specification is significant at 5\% level while all other comparisons
are (marginally) nonsignificant. Hence, the restricted mean-variance 
distribution is adopted here which also has the lowest BIC. 


\setkeys{Gin}{width = 0.65\textwidth}
\begin{figure}[t!]
\centering
%  
<<fig-verbalTest, echo = FALSE, fig = TRUE, width = 6.5, height = 4.5>>=
par(mar = rep(0, 4))

node <- function(x, y, lab, eps = 0.11, col = mygrays[2]) {
  polygon(x + c(-1, -1, 1, 1, -1) * eps, y + c(-1, 1, 1, -1, -1) * eps, col = col)
  text(x, y, lab)
}
edge <- function(x1, y1, x2, y2, lab, eps = 0.13, length = 0.1, ...) {
  if(x1 == x2) {
    arrows(x1, y1 + eps, x2, y2 - eps, length = length, ...)
    text(mean(c(x1, x2)), mean(c(y1, y2)), lab, pos = 2)
  } else {
    arrows(x1 + eps, y1, x2 - eps, y2, length = length, ...)  
    text(mean(c(x1, x2)), mean(c(y1, y2)), lab, pos = 1)
  }
}

plot(0, 0, xlim = c(0, 1), ylim = c(0, 1), type = "n", axes = FALSE, xlab = "", ylab = "")

node(0.2, 0.2, sprintf("saturated\n(BIC: %s)", format(round(BIC(sat3), digits = 1), nsmall = 1)))
node(0.2, 0.8, sprintf("saturated\nrestricted\n(BIC: %s)", format(round(BIC(satR3), digits = 1), nsmall = 1)))
node(0.8, 0.2, sprintf("mean-variance\n(BIC: %s)", format(round(BIC(mv3), digits = 1), nsmall = 1)))
node(0.8, 0.8, sprintf("mean-variance\nrestricted\n(BIC: %s)", format(round(BIC(mvR3), digits = 1), nsmall = 1)))

edge(0.2, 0.2, 0.2, 0.8, format(round(lrtest(sat3, satR3)[2,5], digits = 3), nsmall = 3))
edge(0.2, 0.8, 0.8, 0.8, format(round(lrtest(satR3, mvR3)[2,5], digits = 3), nsmall = 3))
edge(0.2, 0.2, 0.8, 0.2, format(round(lrtest(sat3, mv3  )[2,5], digits = 3), nsmall = 3))
edge(0.8, 0.2, 0.8, 0.8, format(round(lrtest(mv3 , mvR3 )[2,5], digits = 3), nsmall = 3))

par(mar = c(5, 4, 4, 2) + 0.1)
@
%
\caption{Likelihood ratio test $p$-values and BIC for Rasch mixture models with
  $K = 3$ latent classes and different score distribution specifications for 
  the verbal aggression data. (Arrows denote the direction of nesting towards 
  more restricted models.)
  \label{fig:verbalTest}}
\end{figure}


Figure~\ref{fig:verbalItems} shows the corresponding item profiles.

\begin{itemize}
  \item The latent class in the right panel (with 
    \Sexpr{clsizes[3]}~observations) shows a very regular zig-zag-pattern 
    where for any type of verbally aggressive response actually \dquote{doing} 
    the response is considered more extreme than just \dquote{wanting} to 
    respond a certain way as represented by the higher item parameters for the 
    second item, the \dquote{do-item}, than the first item, the 
    \dquote{want-item}, of each pair. The three types of response (cursing,
    scolding, shouting) are considered increasingly aggressive, regardless of
    the situation (first six items vs.\ last six items). 
  \item The latent class in the middle panel (with 
    \Sexpr{clsizes[2]}~observations) distinguishes more strongly between the 
    types of response. However, the relationship between wanting and doing is 
    reversed for all responses except shouting. It is more difficult to agree to
    the item \dquote{I want to curse/scold} than to the corresponding item 
    \dquote{I do curse/scold}. This could be interpreted as generally more 
    aggressive behavior where one is quick to react a certain way rather than 
    just wanting to react that way. However, shouting is considered a very 
    aggressive response, both in wanting and doing.
  \item The remaining latent class (with \Sexpr{clsizes[1]}~observations 
    considerably smaller), depicted in the left panel, does not distinguish 
    that clearly between response types, situations or wanting vs.\ doing.
\end{itemize}

\setkeys{Gin}{width=\textwidth}
\begin{figure}[b!]
\centering
%
<<VerbalItemPlot, echo = FALSE, fig = TRUE, width = 10, height = 4.3>>=
trellis.par.set(theme = standard.theme(color = FALSE))
xyplot(mvR3)
@ 
%
\caption{Item profiles for the Rasch mixture model with $\hat K = 3$ latent 
  classes using a restricted mean-variance score distribution for the verbal 
  aggression data.
  \label{fig:verbalItems}}
\end{figure}

%% rs <- rowSums(va12$resp2)
%% hist(rs, breaks = 1:12 - 0.5, freq = FALSE)
%% lines(1:13, scoreProbs(satR3)[,1], col = myhcl[1])
%% #lines(1:13, scoreProbs(mvR3)[,1], col = myhcl[2])
%% #lines(1:13, scoreProbs(mv3)[,1], col = myhcl[3])
%% #lines(1:13, scoreProbs(mv3)[,2], col = myhcl[3])
%% #lines(1:13, scoreProbs(mv3)[,3], col = myhcl[3])

The respondents in this study are thus not scalable to one single Rasch-scale 
but instead need several scales to represent them accurately. A Rasch mixture
model with a restricted score distribution is used to estimate the number of 
latent classes. Given that number of classes, any type of score model is 
conceivable. Here, the various versions are all fairly similar and the
restricted mean-variance specification is chosen based on likelihood ratio
tests. Keep in mind that the resulting fits can be substantially different from
each other as shown in the simulation study, in particular for the case of 
impact between DIF classes. The latent classes estimated here differ mainly in 
their preception of the type and the \dquote{want/do}-relationship of a verbally
aggressive response.


\section{Conclusion}\label{sec:conclusion}

Unlike in a single Rasch model, item parameter estimation is not independent of 
the score distribtution in Rasch mixture models. The saturated and mean-variance
specification of the score model are both well-established. A further option is
the new restricted score specification introduced here. In the context of 
testing for DIF, only the restricted score specification should be used as it 
prevents confounding effects of impact on DIF detection while exhibiting 
detection power positively related to DIF effect size. Given the number of 
latent classes, it may be useful to fit the other score models as well, as they
might improve estimation of group membership and therefore estimation of the 
item parameters. The best fitting model can be selected 
via the likelihood ratio test or an information criterion such as the BIC. This
approach enhances the suitability of the Rasch mixture model as a tool for DIF 
detection as additional infomation contained in the score distribution is only 
employed if it contributes to the estimation of latent classes based on
measurement invariance.


%% \section*{Computational details}

%% An implementation of all versions of the Rasch mixture model mentioned here is 
%% freely available under the General Pulic License in the \proglang{R} package 
%% \pkg{psychomix} from the Comprehensive \proglang{R} Archive Network.
%% Accompanying the package at \url{http://CRAN.R-project.org/package=psychomix} 
%% is a vignette containing the simulation results and a replication of the verbal
%% aggression example. 


\section*{Acknowledgments}

This work was supported by the Austrian Ministry of Science BMWF as part of the 
UniInfrastrukturprogramm of the Focal Point Scientific Computing at
Universit{\"a}t Innsbruck.

\nocite{psychomix:Fischer+Molenaar:1995}
\bibliography{psychomix}

\end{document}
